# -*- coding: utf-8 -*-
"""Fireducks_SriramParisa_Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KBL7UgFq6Bf3EnyYOcUDhl66YEhXMApb

# Real-Time Traffic Flow Analysis and Forecasting Using FireDucks

## Project Objective

The aim of this project is to build a robust, real-time traffic flow analysis system that leverages the high-performance capabilities of FireDucksâ€”a compiler-accelerated DataFrame library fully compatible with pandas. By utilizing a real-world traffic dataset from Kaggle, the project will:

- **Accelerate Data Processing:** Replace pandas with FireDucks to drastically reduce data preprocessing time.
- **Extract Insights:** Clean, transform, and analyze traffic data to uncover patterns, trends, and anomalies.
- **Forecast Traffic Flow:** Develop predictive models to anticipate future traffic conditions.
- **Support Decision-Making:** Provide real-time, actionable insights that help urban planners and businesses optimize traffic management and reduce congestion.

This project not only demonstrates the practical benefits of using FireDucks in data-intensive applications but also addresses a critical challenge in modern urban mobility.
"""

!pip install fireducks

print("Hello World!")

import os
import fireducks.pandas as pd  # Using FireDucks for enhanced performance

base_path = "/content/drive/MyDrive/Fireducks/blogathon"
train_file = os.path.join(base_path, "tra_Y_tr.csv")
test_file = os.path.join(base_path, "tra_Y_te.csv")
adj_file = os.path.join(base_path, "tra_adj_mat.csv")

df_train = pd.read_csv(train_file)
df_test = pd.read_csv(test_file)
df_adj = pd.read_csv(adj_file)

print("Training Data (tra_Y_tr.csv):", df_train.shape, df_train.columns.tolist())
print("Test Data (tra_Y_te.csv):", df_test.shape, df_test.columns.tolist())
print("Adjacency Matrix (tra_adj_mat.csv):", df_adj.shape, df_adj.columns.tolist())

import time
import pandas as pd

start_pd = time.time()
df_train_pd = pd.read_csv(train_file)
end_pd = time.time()

print("Pandas - Training Data Shape:", df_train_pd.shape)
print(df_train_pd.head())
print(f"Pandas load time: {end_pd - start_pd:.4f} seconds")

if 'timestamp' in df_train_pd.columns and 'traffic_flow' in df_train_pd.columns:
    df_train_pd['timestamp'] = pd.to_datetime(df_train_pd['timestamp'], errors='coerce')
    df_train_pd['hour'] = df_train_pd['timestamp'].dt.hour
    avg_pd = df_train_pd.groupby('hour')['traffic_flow'].mean().reset_index()
    print("Pandas - Average Traffic Flow by Hour:")
    print(avg_pd)

import fireducks.pandas as fpd
import time

start_fd = time.time()
df_train_fd = fpd.read_csv(train_file)
df_train_fd._evaluate()  # Force evaluation of lazy computations
end_fd = time.time()

print("FireDucks - Training Data Shape:", df_train_fd.shape)
print(df_train_fd.head())
print(f"FireDucks load time: {end_fd - start_fd:.4f} seconds")

if 'timestamp' in df_train_fd.columns and 'traffic_flow' in df_train_fd.columns:
    df_train_fd['timestamp'] = fpd.to_datetime(df_train_fd['timestamp'], errors='coerce')
    df_train_fd['hour'] = df_train_fd['timestamp'].dt.hour
    avg_fd = df_train_fd.groupby('hour')['traffic_flow'].mean().reset_index()
    avg_fd._evaluate()
    print("FireDucks - Average Traffic Flow by Hour:")
    print(avg_fd)

df_train_trad = pd.read_csv(train_file)
df_train_trad_t = df_train_trad.transpose()
df_train_trad_t.columns = [f"Sensor_{i+1}" for i in range(df_train_trad_t.shape[1])]
avg_flow_pd = df_train_trad_t.mean(axis=1).reset_index()
avg_flow_pd.columns = ['Time_Step', 'Avg_Traffic']
print("Traditional pandas - Average Traffic Flow (first 5 rows):")
print(avg_flow_pd.head())

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(12, 6))
sns.lineplot(data=avg_flow_pd, x='Time_Step', y='Avg_Traffic', marker='o')
plt.title("Average Traffic Flow Over Time (Traditional pandas)")
plt.xlabel("Time Step")
plt.ylabel("Average Traffic Flow")
plt.show()

df_train_fd_t = df_train_fd.transpose()
df_train_fd_t.columns = [f"Sensor_{i+1}" for i in range(df_train_fd_t.shape[1])]
df_train_fd_t._evaluate()

start_fd_agg = time.time()
avg_flow_fd = df_train_fd_t.mean(axis=1).reset_index()
avg_flow_fd.columns = ['Time_Step', 'Avg_Traffic']
avg_flow_fd._evaluate()
end_fd_agg = time.time()

print("FireDucks - Average Traffic Flow (first 5 rows):")
print(avg_flow_fd.head())
print(f"FireDucks aggregation time: {end_fd_agg - start_fd_agg:.4f} seconds")

plt.figure(figsize=(12, 6))
sns.lineplot(data=avg_flow_fd, x='Time_Step', y='Avg_Traffic', marker='o', color='orange')
plt.title("Average Traffic Flow Over Time (FireDucks)")
plt.xlabel("Time Step")
plt.ylabel("Average Traffic Flow")
plt.show()

comparison_df = pd.merge(avg_flow_pd, avg_flow_fd, on='Time_Step', suffixes=('_pandas', '_fireducks'))
corr_value = comparison_df['Avg_Traffic_pandas'].corr(comparison_df['Avg_Traffic_fireducks'])
print("Correlation between Pandas and FireDucks average flows:", corr_value)

fig, axes = plt.subplots(2, 2, figsize=(14, 12))
sns.lineplot(ax=axes[0,0], data=avg_flow_pd, x='Time_Step', y='Avg_Traffic', marker='o', label='Pandas')
sns.lineplot(ax=axes[0,0], data=avg_flow_fd, x='Time_Step', y='Avg_Traffic', marker='o', label='FireDucks', color='orange')
axes[0,0].set_title("Average Traffic Flow Over Time")
axes[0,0].legend()

sns.scatterplot(ax=axes[0,1], x='Avg_Traffic_pandas', y='Avg_Traffic_fireducks', data=comparison_df)
min_val = comparison_df[['Avg_Traffic_pandas', 'Avg_Traffic_fireducks']].min().min()
max_val = comparison_df[['Avg_Traffic_pandas', 'Avg_Traffic_fireducks']].max().max()
axes[0,1].plot([min_val, max_val], [min_val, max_val], 'r--', label='Ideal Match')
axes[0,1].set_title("Scatter Comparison: Pandas vs. FireDucks")
axes[0,1].legend()

subset_pd = df_train_trad_t.iloc[:100, :]
sns.heatmap(subset_pd, ax=axes[1,0], cmap="viridis")
axes[1,0].set_title("Pandas: Heatmap (First 100 Time Steps)")

subset_fd = df_train_fd_t.iloc[:100, :].copy()
subset_fd._evaluate()
sns.heatmap(subset_fd, ax=axes[1,1], cmap="magma")
axes[1,1].set_title("FireDucks: Heatmap (First 100 Time Steps)")

plt.tight_layout()
plt.show()

stats_pd = df_train_trad_t.describe().loc[['mean', 'min', 'max']]
stats_fd = df_train_fd_t.describe().loc[['mean', 'min', 'max']]
summary = pd.concat([stats_pd, stats_fd], axis=1, keys=['Pandas', 'FireDucks'])
print("Summary Statistics Comparison:")
print(summary)

import os
import time
import pandas as pd
import fireducks.pandas as fpd  # FireDucks drop-in for pandas

# Define file path and load the small dataset
base_path = "/content/drive/MyDrive/Fireducks/blogathon"
train_file = os.path.join(base_path, "tra_Y_tr.csv")

# Load using both Pandas and FireDucks
df_small_pd = pd.read_csv(train_file)
df_small_fd = fpd.read_csv(train_file)

# Simulate a larger dataset by replicating the small dataset multiple times
replications = 100  # Increase this number for a larger dataset simulation
df_large_pd = pd.concat([df_small_pd]*replications, ignore_index=True)
df_large_fd = fpd.concat([df_small_fd]*replications, ignore_index=True)

# --------- Idea 1: Single Operation Benchmark on Large Dataset ---------
# We'll compute the mean of all numeric columns as an example operation

# Pandas Timing
start_pd = time.time()
result_pd = df_large_pd.mean(numeric_only=True)
end_pd = time.time()

# FireDucks Timing
start_fd = time.time()
result_fd = df_large_fd.mean(numeric_only=True)
result_fd._evaluate()  # Force lazy evaluation in FireDucks
end_fd = time.time()

print("Large Dataset Operation (Mean Calculation):")
print(f"Pandas time: {end_pd - start_pd:.4f} seconds")
print(f"FireDucks time: {end_fd - start_fd:.4f} seconds\n")

# --------- Idea 2: Repeated Heavy Operation (Applymap) ---------
# We'll use an applymap operation that squares each numeric value

# Pandas Stress-Test
iterations = 10
start_pd_loop = time.time()
for i in range(iterations):
    temp_pd = df_large_pd.select_dtypes(include=['number']).applymap(lambda x: x**2)
end_pd_loop = time.time()

# FireDucks Stress-Test
start_fd_loop = time.time()
for i in range(iterations):
    temp_fd = df_large_fd.select_dtypes(include=['number']).applymap(lambda x: x**2)
    temp_fd._evaluate()  # Force evaluation each iteration
end_fd_loop = time.time()

print("Repeated Heavy Operation (applymap) Total Time:")
print(f"Pandas: {end_pd_loop - start_pd_loop:.4f} seconds")
print(f"FireDucks: {end_fd_loop - start_fd_loop:.4f} seconds\n")

# --------- Idea 3: Visual Comparison of Execution Times ---------
import matplotlib.pyplot as plt

methods = ['Pandas (Mean)', 'FireDucks (Mean)', 'Pandas (applymap)', 'FireDucks (applymap)']
times = [end_pd - start_pd, end_fd - start_fd,
         end_pd_loop - start_pd_loop, end_fd_loop - start_fd_loop]

plt.figure(figsize=(8, 6))
plt.bar(methods, times, color=['blue', 'orange', 'blue', 'orange'])
plt.ylabel('Time (seconds)')
plt.title('Performance Comparison: Pandas vs. FireDucks')
plt.show()